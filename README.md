# Analyse de l'Ã‰volution des StÃ©rÃ©otypes au CinÃ©ma (1960-2020)

## ğŸ“Œ PrÃ©sentation du Projet

Ce projet analyse l'Ã©volution des stÃ©rÃ©otypes (sexisme, racisme, homophobie) dans les scripts de films entre **1960 et 2020**, en utilisant des techniques de **traitement automatique du langage (NLP)** et d'analyse quantitative.

### ğŸ¯ Questions de Recherche

1. **Comment la reprÃ©sentation des femmes a-t-elle Ã©voluÃ© dans les dialogues de films ?**
   - FrÃ©quence des mentions femmes vs hommes
   - Ã‰volution des stÃ©rÃ©otypes genrÃ©s (Ã©motionnelle, faible, etc.)
   - Objectification et descriptions physiques

2. **Les groupes ethniques minoritaires sont-ils sous-reprÃ©sentÃ©s ?**
   - Ratio de mentions par groupe ethnique
   - Ã‰volution de la diversitÃ© ethnique
   - StÃ©rÃ©otypes raciaux (criminalitÃ©, exotisme, pauvretÃ©)

3. **Comment les stÃ©rÃ©otypes ont-ils Ã©voluÃ© dÃ©cennie par dÃ©cennie ?**
   - Tendances quantitatives par pÃ©riode
   - Comparaison annÃ©es 1960 vs 2020
   - Points de rupture et Ã©volutions significatives

### ğŸ’¡ HypothÃ¨ses

- **H1** : La reprÃ©sentation des femmes dans les scripts a augmentÃ© depuis 1960, mais reste infÃ©rieure Ã  celle des hommes
- **H2** : Les stÃ©rÃ©otypes nÃ©gatifs sur les femmes (Ã©motionnelle, faible) ont diminuÃ© depuis les annÃ©es 1960
- **H3** : Les groupes ethniques minoritaires sont significativement sous-reprÃ©sentÃ©s, surtout avant les annÃ©es 2000
- **H4** : Les associations nÃ©gatives (criminalitÃ©, pauvretÃ©) avec les minoritÃ©s ont diminuÃ© mais persistent

---

## ğŸ—‚ï¸ Structure du Projet

```
movie_scripts_analysis/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Scripts .txt de Kaggle (non versionnÃ©s)
â”‚   â”œâ”€â”€ meta/               # Fichiers CSV avec mÃ©tadonnÃ©es (titres, annÃ©es)
â”‚   â””â”€â”€ processed/          # DonnÃ©es nettoyÃ©es (scripts_clean.pkl)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ parser.py           # Parsing et nettoyage des scripts (Regex)
â”‚   â”œâ”€â”€ dictionaries.py     # Listes de mots pour sexisme/racisme/homophobie
â”‚   â””â”€â”€ stats_analysis.py   # Calcul des frÃ©quences relatives par dÃ©cennie
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 0_clean_data.ipynb  # TÃ©lÃ©chargement et nettoyage massif
â”‚   â”œâ”€â”€ 1_gender_bias.ipynb # Analyse du sexisme (femmes vs hommes)
â”‚   â””â”€â”€ 2_ethnic_bias.ipynb # Analyse des biais ethniques/raciaux
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/            # Graphiques (Ã©volution par dÃ©cennie, ratios, etc.)
â”‚   â”œâ”€â”€ gender_bias_by_decade.csv
â”‚   â”œâ”€â”€ ethnic_bias_by_decade.csv
â”‚   â””â”€â”€ models/             # ModÃ¨les Word2Vec ou LDA (si utilisÃ©s)
â”‚
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â””â”€â”€ README.md               # Ce fichier
```

---

## ğŸš€ Installation et Utilisation

### 1. Cloner le projet

```bash
git clone <votre-repo>
cd movie_scripts_analysis
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

**DÃ©pendances principales :**
- `pandas`, `numpy` : Manipulation de donnÃ©es
- `matplotlib`, `seaborn` : Visualisation
- `nltk` : Tokenisation et stopwords
- `kagglehub` : TÃ©lÃ©chargement du dataset
- `gensim` : Word embeddings (Word2Vec)
- `spacy` : Lemmatisation avancÃ©e (optionnel)

### 3. Configurer Kaggle API (pour tÃ©lÃ©charger le dataset)

1. CrÃ©er un compte sur [Kaggle](https://www.kaggle.com/)
2. TÃ©lÃ©charger votre clÃ© API : **Profile â†’ API â†’ Create New API Token**
3. Placer le fichier `kaggle.json` dans `~/.kaggle/`

```bash
mkdir -p ~/.kaggle
mv ~/Downloads/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json
```

### 4. ExÃ©cuter les notebooks dans l'ordre

#### a) Nettoyage des donnÃ©es
```bash
jupyter notebook notebooks/0_clean_data.ipynb
```
- TÃ©lÃ©charge le dataset **Movie Scripts Corpus** de Kaggle
- Nettoie les scripts avec Regex
- Sauvegarde `data/processed/scripts_clean.pkl`

#### b) Analyse du sexisme
```bash
jupyter notebook notebooks/1_gender_bias.ipynb
```
- Calcule les frÃ©quences de mentions femmes vs hommes
- Analyse les stÃ©rÃ©otypes genrÃ©s
- GÃ©nÃ¨re des graphiques d'Ã©volution

#### c) Analyse des biais ethniques
```bash
jupyter notebook notebooks/2_ethnic_bias.ipynb
```
- Analyse la reprÃ©sentation des groupes ethniques
- StÃ©rÃ©otypes raciaux (criminalitÃ©, exotisme, pauvretÃ©)
- Co-occurrences de mots

---

## ğŸ“Š MÃ©thodologie

### 1. **Acquisition des DonnÃ©es**

**Source** : [Movie Scripts Corpus (Kaggle)](https://www.kaggle.com/datasets/gufukuro/movie-scripts-corpus)
- **~1500 scripts** de films (formats .txt)
- **MÃ©tadonnÃ©es** : titres, annÃ©es de sortie, genres

**PÃ©riode Ã©tudiÃ©e** : 1960-2020 (6 dÃ©cennies)

### 2. **PrÃ©traitement**

- **Nettoyage Regex** : Suppression des indications scÃ©niques, numÃ©ros de scÃ¨ne
- **Tokenisation** : DÃ©coupage en mots
- **Lemmatisation** : Normalisation (optionnel)
- **Suppression des stopwords** : Mots vides (the, a, is, etc.)

### 3. **Dictionnaires de Mots-ClÃ©s**

CrÃ©ation de listes de mots pour chaque catÃ©gorie de biais :

#### Sexisme (`dictionaries.py`)
```python
GENDER_WORDS = {
    'female': ['woman', 'women', 'girl', 'she', 'her', ...],
    'male': ['man', 'men', 'boy', 'he', 'him', ...]
}

GENDER_STEREOTYPES = {
    'female_negative': ['emotional', 'hysterical', 'weak', ...],
    'female_objectification': ['beautiful', 'sexy', 'hot', ...],
    'male_stereotypes': ['strong', 'brave', 'dominant', ...]
}
```

#### Racisme (`dictionaries.py`)
```python
ETHNICITY_WORDS = {
    'african_american': ['black', 'african', ...],
    'asian': ['asian', 'chinese', 'japanese', ...],
    'hispanic': ['hispanic', 'latino', 'mexican', ...],
    'white': ['white', 'caucasian', ...]
}

RACIAL_STEREOTYPES = {
    'criminal': ['thug', 'gangster', 'violent', ...],
    'exotic': ['exotic', 'mysterious', 'oriental', ...],
    'poverty': ['poor', 'ghetto', 'slum', ...]
}
```

### 4. **Calcul des FrÃ©quences Relatives**

Pour chaque dÃ©cennie et catÃ©gorie :

$$
\text{FrÃ©quence relative} = \frac{\text{Nb occurrences mot-clÃ©}}{\text{Nb total de mots}} \times 1000
$$

**Exemple** : Si "woman" apparaÃ®t 50 fois dans un script de 10 000 mots â†’ frÃ©quence = 5 pour 1000 mots.

### 5. **Analyse Statistique**

- **Ratios** : Femmes/Hommes, MinoritÃ©s/Blancs
- **Tendances** : RÃ©gression linÃ©aire pour dÃ©tecter augmentation/diminution
- **Comparaisons** : Tests de significativitÃ© (t-tests) entre dÃ©cennies

### 6. **Visualisations**

- Histogrammes par dÃ©cennie
- Courbes d'Ã©volution temporelle
- Boxplots de distribution
- Heatmaps de corrÃ©lations

---

## ğŸ“ˆ RÃ©sultats Attendus

### Graphiques GÃ©nÃ©rÃ©s

1. **`gender_mentions_by_decade.png`** : Barres comparant mentions femmes vs hommes
2. **`gender_ratio_evolution.png`** : Courbe du ratio femmes/hommes
3. **`gender_stereotypes.png`** : Ã‰volution des stÃ©rÃ©otypes nÃ©gatifs
4. **`ethnic_mentions_by_decade.png`** : Mentions des groupes ethniques
5. **`racial_stereotypes.png`** : Associations nÃ©gatives par dÃ©cennie

### Fichiers de RÃ©sultats

- **`results/gender_bias_by_decade.csv`** : Statistiques agrÃ©gÃ©es par dÃ©cennie
- **`results/ethnic_bias_by_decade.csv`** : Biais ethniques par dÃ©cennie
- **`results/gender_bias_by_film.csv`** : MÃ©triques dÃ©taillÃ©es par film

---

## ğŸ” Pistes d'AmÃ©lioration

### Analyses ComplÃ©mentaires

1. **Word Embeddings (Word2Vec)**
   - EntraÃ®ner un modÃ¨le par dÃ©cennie
   - Analyser l'Ã©volution sÃ©mantique de "woman", "black", etc.
   - Visualiser avec t-SNE

2. **Topic Modeling (LDA)**
   - Identifier les thÃ¨mes dominants par dÃ©cennie
   - Tracker l'Ã©volution des sujets liÃ©s aux minoritÃ©s

3. **Analyse de Graphes**
   - Graphes d'interaction entre personnages
   - Mesure de centralitÃ© par genre et ethnicitÃ©

4. **Analyse de Sentiments**
   - PolaritÃ© des dialogues associÃ©s Ã  chaque groupe
   - Ã‰volution du ton (positif/nÃ©gatif)

5. **IntersectionnalitÃ©**
   - Croisement genre Ã— ethnicitÃ©
   - Analyse spÃ©cifique des femmes noires, asiatiques, etc.

---

## ğŸ“š RÃ©fÃ©rences

### Datasets

- [Movie Scripts Corpus (Kaggle)](https://www.kaggle.com/datasets/gufukuro/movie-scripts-corpus)
- [CMU Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/)

### LittÃ©rature AcadÃ©mique

1. **Linguistique Computationnelle**
   - Cours Maria Boritchev (TÃ©lÃ©com Paris)
   - Jurafsky & Martin, *Speech and Language Processing*

2. **Biais dans les MÃ©dias**
   - Geena Davis Institute on Gender in Media
   - USC Annenberg Inclusion Initiative

3. **NLP et Biais**
   - Bolukbasi et al. (2016), *Man is to Computer Programmer as Woman is to Homemaker?*
   - Caliskan et al. (2017), *Semantics derived automatically from language corpora*

---

## ğŸ‘¥ Auteurs

- **Antoine Ollivier** - TÃ©lÃ©com Paris (Promo 2026)
- **Projet** : Computational Science (CrÃ©neau D)

---

## ğŸ“ Licence

Projet acadÃ©mique - TÃ©lÃ©com Paris 2025

---

## ğŸ†˜ Contact

Pour toute question :
- Email : antoine.ollivier@telecom-paris.fr
- GitHub : [Votre pseudo GitHub]

---

**DerniÃ¨re mise Ã  jour** : Janvier 2026
